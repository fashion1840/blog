#!/bin/bash
set -e

# åŠ è½½ç¯å¢ƒå˜é‡
if [ -f "/root/.openclaw/workspace/blog-deploy/.env" ]; then
    set -a
    source /root/.openclaw/workspace/blog-deploy/.env
    set +a
fi

if [ -z "$TAVILY_API_KEY" ] || [ -z "$GITHUB_TOKEN" ]; then
    echo "é”™è¯¯: è¯·ç¡®ä¿ .env æ–‡ä»¶ä¸­è®¾ç½®äº† TAVILY_API_KEY å’Œ GITHUB_TOKEN"
    exit 1
fi

WORKDIR="/root/.openclaw/workspace/blog-deploy"
TIMESTAMP=$(date +%Y-%m-%d)
YEAR=$(date +%Y)
MONTH=$(date +%m)
DAY=$(date +%d)
POST_DIR="${WORKDIR}/${YEAR}/${MONTH}/${DAY}/daily-digest"
MARKDOWN_FILE="${POST_DIR}/index.md"

mkdir -p "$POST_DIR"

echo "ğŸ” å¼€å§‹æœé›†24å°æ—¶å†…çš„çƒ­é—¨ä¿¡æ¯..."

# Hacker News
node "/root/.openclaw/workspace/skills/tavily-search/scripts/search.mjs" \
  "Hacker News top stories last 24 hours" \
  -n 8 --topic news --days 1 \
  > "${POST_DIR}/hackernews_raw.txt" 2>/dev/null || echo "" > "${POST_DIR}/hackernews_raw.txt"

# Reddit
node "/root/.openclaw/workspace/skills/tavily-search/scripts/search.mjs" \
  "Reddit popular posts r/technology r/programming past 24 hours" \
  -n 8 --topic news --days 1 \
  > "${POST_DIR}/reddit_raw.txt" 2>/dev/null || echo "" > "${POST_DIR}/reddit_raw.txt"

# Product Hunt
node "/root/.openclaw/workspace/skills/tavily-search/scripts/search.mjs" \
  "Product Hunt latest launches past 24 hours" \
  -n 8 --topic news --days 1 \
  > "${POST_DIR}/producthunt_raw.txt" 2>/dev/null || echo "" > "${POST_DIR}/producthunt_raw.txt"

echo "âœ… æœç´¢å®Œæˆï¼Œç”ŸæˆMarkdown..."

python3 - << 'PYEOF' > "${MARKDOWN_FILE}"
import re
from datetime import datetime
import os

# ä»ç¯å¢ƒå˜é‡è·å–è·¯å¾„
post_dir = os.environ.get('POST_DIR', '/tmp')
year = os.path.basename(os.path.dirname(os.path.dirname(post_dir)))
month = os.path.basename(os.path.dirname(post_dir))
day = os.path.basename(post_dir.rstrip('/'))

ts = datetime.now().strftime("%Y-%m-%d")
tnow = datetime.now().strftime("%H:%M")

# Hexo Front Matter - ç¬¦åˆä½ çš„åšå®¢ç»“æ„
front_matter = f"""---
title: æ¯æ—¥ç§‘æŠ€æ‘˜è¦ - {ts}
date: {ts} {tnow}
tags: [daily-digest, tech-news]
categories: [ç§‘æŠ€èµ„è®¯]
layout: post
---
"""

md = f"{front_matter}# æ¯æ—¥ç§‘æŠ€æ‘˜è¦\n\n> ğŸ“… é‡‡é›†æ—¥æœŸï¼š{ts} {tnow} UTC\n> ğŸ“Š æ•°æ®æ¥æºï¼šHacker News, Reddit, Product Hunt\n\n---\n\n"

def clean_text(text, max_len=300):
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    if len(text) > max_len:
        text = text[:max_len].rsplit(' ', 1)[0] + '...'
    return text

def parse_tavily(text):
    items = []
    answer_match = re.search(r'## Answer\s+(.*?)(?=\n##|\Z)', text, re.DOTALL)
    overall_summary = clean_text(answer_match.group(1), 500) if answer_match else ""

    sources_match = re.search(r'## Sources\s+(.*)', text, re.DOTALL)
    if not sources_match:
        return [], overall_summary

    src = sources_match.group(1)
    entries = re.split(r'(?=^- \*\*)', src, flags=re.MULTILINE)

    for entry in entries:
        entry = entry.strip()
        if not entry:
            continue

        title_match = re.search(r'- \*\*(.*?)\*\*', entry)
        title = title_match.group(1).strip() if title_match else 'æ— æ ‡é¢˜'

        rel_match = re.search(r'\(relevance:\s*(\d+)%\)', entry)
        relevance = rel_match.group(1) if rel_match else None

        url_match = re.search(r'(https?://[^\s\)]+)', entry)
        url = url_match.group(1) if url_match else '#'

        desc_lines = []
        for line in entry.split('\n')[1:]:
            line = line.strip()
            if line and not line.startswith('-') and 'http' not in line and 'relevance' not in line.lower():
                desc_lines.append(line)
        description = clean_text(' '.join(desc_lines), 200) if desc_lines else ""

        items.append({
            'title': title,
            'url': url,
            'relevance': relevance,
            'description': description
        })
    return items, overall_summary

def section(icon, name, fn):
    try:
        raw = open(fn).read()
        if not raw.strip():
            return f"## {icon} {name}\n\n*æš‚æ— æ•°æ®*\n\n"
        items, summary = parse_tavily(raw)

        if not items:
            return f"## {icon} {name}\n\n*æš‚æ— æ–°å†…å®¹*\n\n"

        out = f"## {icon} {name}\n\n"

        for i, it in enumerate(items[:8], 1):
            out += f"### {i}. {it['title']}\n\n"
            if it['description']:
                out += f"{it['description']}\n\n"
            out += f"[ğŸ”— é˜…è¯»åŸæ–‡]({it['url']})\n\n"
            if it['relevance']:
                out += f"*ç›¸å…³æ€§ï¼š{it['relevance']}%*\n\n"
            out += "---\n\n"

        if summary:
            out += f"**ğŸ“Œ ä»Šæ—¥æ‘˜è¦**ï¼š{summary}\n\n"

        return out
    except Exception as e:
        return f"## {icon} {name}\n\n*è¯»å–å¤±è´¥*\n\n"

md += section("ğŸ“°", "Hacker News çƒ­é—¨", f"{post_dir}/hackernews_raw.txt")
md += section("ğŸ¤–", "Reddit ç§‘æŠ€/ç¼–ç¨‹", f"{post_dir}/reddit_raw.txt")
md += section("ğŸš€", "Product Hunt æ–°å“", f"{post_dir}/producthunt_raw.txt")

md += "---\n\n*æœ¬æ‘˜è¦ç”± OpenClaw è‡ªåŠ¨ç”Ÿæˆï¼Œæ¯æ—¥æ›´æ–°*"

print(md)
PYEOF

echo "ğŸ“ å·²ç”Ÿæˆ ${MARKDOWN_FILE}"

cd "$WORKDIR"
git pull --rebase origin master || true
git add "${YEAR}/${MONTH}/${DAY}/"
git commit -m "æ·»åŠ æ¯æ—¥æ‘˜è¦ ${ts}" || true
git push origin master

echo "ğŸš€ æ¨é€å®Œæˆï¼"