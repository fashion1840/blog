#!/bin/bash
set -e

# åŠ è½½ç¯å¢ƒå˜é‡
if [ -f "/root/.openclaw/workspace/blog-deploy/.env" ]; then
    set -a
    source /root/.openclaw/workspace/blog-deploy/.env
    set +a
fi

if [ -z "$TAVILY_API_KEY" ] || [ -z "$GITHUB_TOKEN" ]; then
    echo "é”™è¯¯: è¯·ç¡®ä¿ .env æ–‡ä»¶ä¸­è®¾ç½®äº† TAVILY_API_KEY å’Œ GITHUB_TOKEN"
    exit 1
fi

WORKDIR="/root/.openclaw/workspace/blog-deploy"
OUTPUT_DIR="${WORKDIR}/posts"
TIMESTAMP=$(date +%Y-%m-%d)
MARKDOWN_FILE="${OUTPUT_DIR}/${TIMESTAMP}-daily-digest.md"

mkdir -p "$OUTPUT_DIR"

echo "ğŸ” å¼€å§‹æœé›†24å°æ—¶å†…çš„çƒ­é—¨ä¿¡æ¯..."

# Hacker News
node "/root/.openclaw/workspace/skills/tavily-search/scripts/search.mjs" \
  "Hacker News top stories last 24 hours" \
  -n 8 --topic news --days 1 \
  > "${OUTPUT_DIR}/hackernews_raw.txt" 2>/dev/null || echo "" > "${OUTPUT_DIR}/hackernews_raw.txt"

# Reddit
node "/root/.openclaw/workspace/skills/tavily-search/scripts/search.mjs" \
  "Reddit popular posts r/technology r/programming past 24 hours" \
  -n 8 --topic news --days 1 \
  > "${OUTPUT_DIR}/reddit_raw.txt" 2>/dev/null || echo "" > "${OUTPUT_DIR}/reddit_raw.txt"

# Product Hunt
node "/root/.openclaw/workspace/skills/tavily-search/scripts/search.mjs" \
  "Product Hunt latest launches past 24 hours" \
  -n 8 --topic news --days 1 \
  > "${OUTPUT_DIR}/producthunt_raw.txt" 2>/dev/null || echo "" > "${OUTPUT_DIR}/producthunt_raw.txt"

echo "âœ… æœç´¢å®Œæˆï¼Œç”ŸæˆMarkdown..."

python3 - << 'PYEOF' > "${MARKDOWN_FILE}"
import re
from datetime import datetime

ts = datetime.now().strftime("%Y-%m-%d")
tnow = datetime.now().strftime("%H:%M")

# Hexo Front Matterï¼ˆå…¼å®¹æ ‡å‡†Hexoä¸»é¢˜ï¼‰
front_matter = f"""---
title: æ¯æ—¥ç§‘æŠ€æ‘˜è¦ - {ts}
date: {ts} {tnow}
tags: [daily-digest, tech-news]
categories: [ç§‘æŠ€èµ„è®¯]
description: æ¯æ—¥ç§‘æŠ€æ–°é—»æ‘˜è¦ï¼ŒåŒ…å«Hacker Newsã€Redditå’ŒProduct Huntçš„æœ€æ–°çƒ­é—¨å†…å®¹
---
"""

md = f"{front_matter}# æ¯æ—¥ç§‘æŠ€æ‘˜è¦\n\n> ğŸ“… é‡‡é›†æ—¥æœŸï¼š{ts} {tnow} UTC\n> ğŸ“Š æ•°æ®æ¥æºï¼šHacker News, Reddit, Product Hunt\n\n---\n\n"

def clean_text(text, max_len=300):
    """æ¸…ç†æ–‡æœ¬ï¼Œæå–å‰max_lenå­—ç¬¦ä½œä¸ºæ‘˜è¦"""
    # ç§»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    # æˆªæ–­åˆ°åˆé€‚é•¿åº¦
    if len(text) > max_len:
        text = text[:max_len].rsplit(' ', 1)[0] + '...'
    return text

def parse_tavily(text):
    items = []
    # æå–Answerä½œä¸ºæ•´ä½“æ‘˜è¦
    answer_match = re.search(r'## Answer\s+(.*?)(?=\n##|\Z)', text, re.DOTALL)
    overall_summary = clean_text(answer_match.group(1), 500) if answer_match else ""

    # æå–Sourcesä¸­çš„æ¯ä¸€æ¡
    sources_match = re.search(r'## Sources\s+(.*)', text, re.DOTALL)
    if not sources_match:
        return [], overall_summary

    src = sources_match.group(1)
    # åˆ†å‰²æ¡ç›®
    entries = re.split(r'(?=^- \*\*)', src, flags=re.MULTILINE)

    for entry in entries:
        entry = entry.strip()
        if not entry:
            continue

        # æ ‡é¢˜
        title_match = re.search(r'- \*\*(.*?)\*\*', entry)
        title = title_match.group(1).strip() if title_match else 'æ— æ ‡é¢˜'

        # ç›¸å…³æ€§
        rel_match = re.search(r'\(relevance:\s*(\d+)%\)', entry)
        relevance = rel_match.group(1) if rel_match else None

        # URL
        url_match = re.search(r'(https?://[^\s\)]+)', entry)
        url = url_match.group(1) if url_match else '#'

        # æå–æè¿°ï¼ˆå»æ‰æ ‡é¢˜è¡Œã€URLè¡Œã€relevanceè¡Œä¹‹åçš„å†…å®¹ï¼‰
        desc_lines = []
        for line in entry.split('\n')[1:]:
            line = line.strip()
            if line and not line.startswith('-') and 'http' not in line and 'relevance' not in line.lower():
                desc_lines.append(line)
        description = clean_text(' '.join(desc_lines), 200) if desc_lines else ""

        items.append({
            'title': title,
            'url': url,
            'relevance': relevance,
            'description': description
        })
    return items, overall_summary

def section(icon, name, fn):
    try:
        raw = open(fn).read()
        if not raw.strip():
            return f"## {icon} {name}\n\n*æš‚æ— æ•°æ®*\n\n"
        items, summary = parse_tavily(raw)

        if not items:
            return f"## {icon} {name}\n\n*æš‚æ— æ–°å†…å®¹*\n\n"

        out = f"## {icon} {name}\n\n"

        for i, it in enumerate(items[:8], 1):
            out += f"### {i}. {it['title']}\n\n"
            if it['description']:
                out += f"{it['description']}\n\n"
            out += f"[ğŸ”— é˜…è¯»åŸæ–‡]({it['url']})\n\n"
            if it['relevance']:
                out += f"*ç›¸å…³æ€§ï¼š{it['relevance']}%*\n\n"
            out += "---\n\n"

        if summary:
            out += f"**ğŸ“Œ ä»Šæ—¥æ‘˜è¦**ï¼š{summary}\n\n"

        return out
    except Exception as e:
        return f"## {icon} {name}\n\n*è¯»å–å¤±è´¥*\n\n"

md += section("ğŸ“°", "Hacker News çƒ­é—¨", "/root/.openclaw/workspace/blog-deploy/posts/hackernews_raw.txt")
md += section("ğŸ¤–", "Reddit ç§‘æŠ€/ç¼–ç¨‹", "/root/.openclaw/workspace/blog-deploy/posts/reddit_raw.txt")
md += section("ğŸš€", "Product Hunt æ–°å“", "/root/.openclaw/workspace/blog-deploy/posts/producthunt_raw.txt")

md += "---\n\n*æœ¬æ‘˜è¦ç”± OpenClaw è‡ªåŠ¨ç”Ÿæˆï¼Œæ¯æ—¥æ›´æ–°*"

print(md)
PYEOF

echo "ğŸ“ å·²ç”Ÿæˆ ${MARKDOWN_FILE}"

cd "$WORKDIR"
git pull --rebase origin master || true
git add posts/
git commit -m "æ›´æ–°æ¯æ—¥æ‘˜è¦ ${TIMESTAMP}ï¼ˆHexoæ ¼å¼+ä¸­æ–‡æ‘˜è¦ï¼‰" || true
git push origin master

echo "ğŸš€ æ¨é€å®Œæˆï¼"